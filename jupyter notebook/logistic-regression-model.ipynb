{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "874ac3e0-12f4-4c28-9f6d-bec9aafa09f5",
   "metadata": {},
   "source": [
    " # Logistic Regression\n",
    " ## 1. Import data to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "d9c395ce-10ea-4d19-95dd-bd948314c56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['MNCAATourneyCompactResults', 'MNCAATourneySeeds', 'MRegularSeasonCompactResults', 'MSeasons', 'MTeams', 'WNCAATourneyCompactResults', 'WNCAATourneySeeds', 'WRegularSeasonCompactResults', 'WSeasons', 'WTeams', 'MNCAATourneyDetailedResults', 'MRegularSeasonDetailedResults', 'WNCAATourneyDetailedResults', 'WRegularSeasonDetailedResults', 'Cities', 'MGameCities', 'WGameCities', 'MMasseyOrdinals', 'Conferences', 'MConferenceTourneyGames', 'MNCAATourneySeedRoundSlots', 'MNCAATourneySlots', 'MSecondaryTourneyCompactResults', 'MSecondaryTourneyTeams', 'MTeamCoaches', 'MTeamConferences', 'MTeamSpellings', 'WConferenceTourneyGames', 'WNCAATourneySlots', 'WSecondaryTourneyCompactResults', 'WSecondaryTourneyTeams', 'WTeamConferences', 'WTeamSpellings'])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # import the data\n",
    "    current_dir = os.getcwd()\n",
    "    basics_dir_path = os.path.join(current_dir, '..', 'data', 'section_1_basics')\n",
    "    team_box_scores_dir_path = os.path.join(current_dir, '..', 'data', 'section_2_team_box_scores')\n",
    "    geography_dir_path = os.path.join(current_dir, '..', 'data', 'section_3_geography')\n",
    "    public_rankings_dir_path = os.path.join(current_dir, '..', 'data', 'section_4_public_rankings')\n",
    "    supplements_dir_path = os.path.join(current_dir, '..','data', 'section_5_supplements')\n",
    "\n",
    "    dfs = {}\n",
    "    for path in [basics_dir_path, team_box_scores_dir_path, geography_dir_path, public_rankings_dir_path, supplements_dir_path]:\n",
    "        for filename in os.listdir(path):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                filepath = os.path.join(path, filename)\n",
    "                df_name = filename[:-4]  # Remove the .csv extension\n",
    "                dfs[df_name] = pd.read_csv(filepath)\n",
    "    return dfs  \n",
    "dfs = load_data()\n",
    "dfs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53021f6f-848b-4913-8f74-0e844032f8aa",
   "metadata": {},
   "source": [
    " ## 2. For this model we are going to use the data from `MRegularSeasonCompactResults.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "722574e8-af70-4c7c-b897-6c04e6184be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "games = dfs['MRegularSeasonCompactResults']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fd0d41-0f89-4ebc-bcd8-63213f563ae0",
   "metadata": {},
   "source": [
    " ## 3. Create a dataframe containing the regular game statistics for each match of each season. \n",
    " #### Aggregate the data see the average points scored, average points scored against, and win percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "8de153fb-c8c3-41de-b411-995f3a5d5080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_team_stats(df):\n",
    "    winning_stats = df[['Season', 'WTeamID', 'WScore', 'LScore']].rename(\n",
    "    columns={'WTeamID': 'TeamID', 'WScore': 'PointsFor', 'LScore': 'PointsAgainst'}\n",
    "    )\n",
    "    winning_stats['Win'] = 1\n",
    "\n",
    "    losing_stats = df[['Season', 'LTeamID', 'LScore', 'WScore']].rename(\n",
    "        columns={'LTeamID': 'TeamID', 'LScore': 'PointsFor', 'WScore': 'PointsAgainst'}\n",
    "    )\n",
    "    losing_stats['Win'] = 0\n",
    "\n",
    "    all_stats = pd.concat([winning_stats, losing_stats])\n",
    "\n",
    "    # all_stats['MarginOfVictory'] = all_stats['PointsFor'] - all_stats['PointsAgainst']\n",
    "\n",
    "    # aggregate data to see averaage points for and against, and win percentage\n",
    "    team_stats = all_stats.groupby(['Season', 'TeamID']).agg(\n",
    "    avg_points_for=('PointsFor', 'mean'),\n",
    "    avg_points_against=('PointsAgainst', 'mean'),\n",
    "    win_pct=('Win', 'mean')\n",
    "    # avg_margin_of_victory=('MarginOfVictory', 'mean')  # Add margin of victory\n",
    "    ).reset_index()\n",
    "\n",
    "    return team_stats\n",
    "\n",
    "team_stats = build_team_stats(games)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2039701c-1155-4d3c-8163-3ee9ff086339",
   "metadata": {},
   "source": [
    " ## 4. Create dataframe containing the matchups and merge in data from the statistics dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "8a9d6cf1-8800-444d-9e70-b7de29c4461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matchups(games, team_stats, dfs):\n",
    "    # Create labeled matchup data from real games\n",
    "    matchups = games[['Season', 'WTeamID', 'LTeamID']].copy()\n",
    "    matchups['Team1ID'] = matchups['WTeamID']\n",
    "    matchups['Team2ID'] = matchups['LTeamID']\n",
    "    matchups['Team1Won'] = 1\n",
    "\n",
    "    # Merge in features for both teams\n",
    "    for i in [1, 2]:\n",
    "        matchups = matchups.merge(\n",
    "            team_stats,\n",
    "            how='left',\n",
    "            left_on=['Season', f'Team{i}ID'],\n",
    "            right_on=['Season', 'TeamID']\n",
    "        )\n",
    "        matchups = matchups.rename(columns={\n",
    "            'avg_points_for': f'Team{i}_avg_points_for',\n",
    "            'avg_points_against': f'Team{i}_avg_points_against',\n",
    "            'win_pct': f'Team{i}_win_pct'\n",
    "            # 'avg_margin_of_victory': f'Team{i}_avg_margin_of_victory'  # Add margin of victory\n",
    "        })\n",
    "        matchups.drop(columns=['TeamID'], inplace=True)\n",
    "\n",
    "    # randomly swap Team1 and Team2 \n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Create a random boolean array: True means \"swap\"\n",
    "    swap_mask = np.random.rand(len(matchups)) < 0.5\n",
    "\n",
    "    # Swap team IDs by matching the mask\n",
    "    matchups.loc[swap_mask, ['Team1ID', 'Team2ID']] = matchups.loc[swap_mask, ['Team2ID', 'Team1ID']].values\n",
    "    for feature in ['avg_points_for', 'avg_points_against', 'win_pct']:\n",
    "        team1_feature = f'Team1_{feature}'\n",
    "        team2_feature = f'Team2_{feature}'\n",
    "        matchups.loc[swap_mask, [team1_feature, team2_feature]] = matchups.loc[swap_mask, [team2_feature, team1_feature]].values\n",
    "\n",
    "    # Set the target: 1 if original Team1 won, 0 if swapped\n",
    "    matchups['Team1Won'] = (~swap_mask).astype(int)\n",
    "\n",
    "    # add ordinal rankings to matchups\n",
    "    # matchups = merge_rankings(matchups, dfs)\n",
    "\n",
    "    return matchups\n",
    "\n",
    "matchups = generate_matchups(games, team_stats, dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3afcf-e9af-4548-8639-abc74ae1cb9b",
   "metadata": {},
   "source": [
    "\n",
    " ## 5. Build and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "8982f75b-2fb6-44e0-be35-8ace9f2b0f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.7421\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_cols = [\n",
    "    'Team1_avg_points_for', 'Team1_avg_points_against', 'Team1_win_pct',\n",
    "    'Team2_avg_points_for', 'Team2_avg_points_against', 'Team2_win_pct'\n",
    "]\n",
    "\n",
    "X = matchups[feature_cols]\n",
    "y = matchups['Team1Won']\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This model gives an accuracy of 74.21%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723abdd8-5f5d-4221-bff6-bab6f92fa005",
   "metadata": {},
   "source": [
    " ## 6. Add margin of victory to team stats to improve the model\n",
    " The margin of victory is the difference between the points scored by the winning team and the points scored by the losing team.\n",
    " This is a common feature used in sports analytics to predict the outcome of games.\n",
    " The margin of victory is a good predictor of the outcome of a game because it takes into account the strength of both teams.\n",
    " A team that wins by a large margin is likely to be stronger than a team that wins by a small margin.\n",
    "\n",
    " To do this, we will add the margin of victory to the team stats dataframe and then re-run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_team_stats(df):\n",
    "    winning_stats = df[['Season', 'WTeamID', 'WScore', 'LScore']].rename(\n",
    "    columns={'WTeamID': 'TeamID', 'WScore': 'PointsFor', 'LScore': 'PointsAgainst'}\n",
    "    )\n",
    "    winning_stats['Win'] = 1\n",
    "\n",
    "    losing_stats = df[['Season', 'LTeamID', 'LScore', 'WScore']].rename(\n",
    "        columns={'LTeamID': 'TeamID', 'LScore': 'PointsFor', 'WScore': 'PointsAgainst'}\n",
    "    )\n",
    "    losing_stats['Win'] = 0\n",
    "\n",
    "    all_stats = pd.concat([winning_stats, losing_stats])\n",
    "\n",
    "    all_stats['MarginOfVictory'] = all_stats['PointsFor'] - all_stats['PointsAgainst']\n",
    "\n",
    "    # aggregate data to see averaage points for and against, and win percentage\n",
    "    team_stats = all_stats.groupby(['Season', 'TeamID']).agg(\n",
    "    avg_points_for=('PointsFor', 'mean'),\n",
    "    avg_points_against=('PointsAgainst', 'mean'),\n",
    "    win_pct=('Win', 'mean'),\n",
    "    avg_margin_of_victory=('MarginOfVictory', 'mean')  # Add margin of victory\n",
    "    ).reset_index()\n",
    "\n",
    "    return team_stats\n",
    "\n",
    "team_stats = build_team_stats(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matchups(games, team_stats, dfs):\n",
    "    # Create labeled matchup data from real games\n",
    "    matchups = games[['Season', 'WTeamID', 'LTeamID']].copy()\n",
    "    matchups['Team1ID'] = matchups['WTeamID']\n",
    "    matchups['Team2ID'] = matchups['LTeamID']\n",
    "    matchups['Team1Won'] = 1\n",
    "\n",
    "    # Merge in features for both teams\n",
    "    for i in [1, 2]:\n",
    "        matchups = matchups.merge(\n",
    "            team_stats,\n",
    "            how='left',\n",
    "            left_on=['Season', f'Team{i}ID'],\n",
    "            right_on=['Season', 'TeamID']\n",
    "        )\n",
    "        matchups = matchups.rename(columns={\n",
    "            'avg_points_for': f'Team{i}_avg_points_for',\n",
    "            'avg_points_against': f'Team{i}_avg_points_against',\n",
    "            'win_pct': f'Team{i}_win_pct',\n",
    "            'avg_margin_of_victory': f'Team{i}_avg_margin_of_victory'  # Add margin of victory\n",
    "        })\n",
    "        matchups.drop(columns=['TeamID'], inplace=True)\n",
    "\n",
    "    # randomly swap Team1 and Team2 \n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Create a random boolean array: True means \"swap\"\n",
    "    swap_mask = np.random.rand(len(matchups)) < 0.5\n",
    "\n",
    "    # Swap team IDs by matching the mask\n",
    "    matchups.loc[swap_mask, ['Team1ID', 'Team2ID']] = matchups.loc[swap_mask, ['Team2ID', 'Team1ID']].values\n",
    "    for feature in ['avg_points_for', 'avg_points_against', 'win_pct']:\n",
    "        team1_feature = f'Team1_{feature}'\n",
    "        team2_feature = f'Team2_{feature}'\n",
    "        matchups.loc[swap_mask, [team1_feature, team2_feature]] = matchups.loc[swap_mask, [team2_feature, team1_feature]].values\n",
    "\n",
    "    # Set the target: 1 if original Team1 won, 0 if swapped\n",
    "    matchups['Team1Won'] = (~swap_mask).astype(int)\n",
    "\n",
    "    # add ordinal rankings to matchups\n",
    "    # matchups = merge_rankings(matchups, dfs)\n",
    "\n",
    "    return matchups\n",
    "\n",
    "matchups = generate_matchups(games, team_stats, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "933cfd3c-83ed-4384-87ea-fa13359b8ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy with Margin of Victory: 0.7419\n"
     ]
    }
   ],
   "source": [
    "# Update feature columns to include margin of victory\n",
    "feature_cols = [\n",
    "    'Team1_avg_points_for', 'Team1_avg_points_against', 'Team1_win_pct', 'Team1_avg_margin_of_victory',\n",
    "    'Team2_avg_points_for', 'Team2_avg_points_against', 'Team2_win_pct', 'Team2_avg_margin_of_victory'\n",
    "]\n",
    "# Build feature matrix and labels\n",
    "X = matchups[feature_cols]\n",
    "y = matchups['Team1Won']\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Set Accuracy with Margin of Victory: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e741c035-8a7c-41fd-9f8e-8411c8201055",
   "metadata": {},
   "source": [
    "\n",
    " The model with the margin of victory feature has an accuracy of 74.19%.\n",
    " This is a decrease in performance for the model without the margin of victory feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1055eecb-d6d3-4273-b240-e42d703688c8",
   "metadata": {},
   "source": [
    "\n",
    " ## 6. Add Ranking Features\n",
    " We are going to use data from this file `MMasseyOrdinals.csv` to add ranking features to the model.\n",
    " This file contains the rankings of each team for each season. We will use the rankings to create new features for the model.\n",
    " The features we will create are:\n",
    " - Team1Rank: The ranking of Team1 for the season\n",
    " - Team2Rank: The ranking of Team2 for the season\n",
    " - Team1RankDiff: The difference between the rankings of Team1 and Team2\n",
    " - Team1RankDiffAbs: The absolute difference between the rankings of Team1 and Team2\n",
    " - Team1RankDiffPct: The percentage difference between the rankings of Team1 and Team2\n",
    " - Team1RankDiffPctAbs: The absolute percentage difference between the rankings of Team1 and Team2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_rankings(matchups, dfs):\n",
    "    # Load the rankings data\n",
    "    rankings = dfs['MMasseyOrdinals']\n",
    "\n",
    "    # keep only the relevant columns\n",
    "    rankings = rankings[['Season', 'TeamID', 'OrdinalRank']]\n",
    "    \n",
    "    # keep the average ranking by season for each team\n",
    "    rankings = rankings.groupby(['Season', 'TeamID']).agg(\n",
    "        OrdinalRank=('OrdinalRank', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Add ordinal rankings for both Team1 and Team2\n",
    "    for i in [1, 2]:\n",
    "        matchups = matchups.merge(\n",
    "            rankings, how='left',\n",
    "            left_on=['Season', f'Team{i}ID'],\n",
    "            right_on=['Season', 'TeamID']\n",
    "        )\n",
    "        matchups = matchups.drop(columns=['TeamID'])\n",
    "        matchups = matchups.rename(columns={'OrdinalRank': f'Team{i}Rank'})\n",
    "\n",
    "    # drop records with missing values in the rankings\n",
    "    matchups = matchups.dropna(subset=['Team1Rank', 'Team2Rank'])\n",
    "\n",
    "    # calculate rank difference\n",
    "    matchups['Team1RankDiff'] = matchups['Team1Rank'] - matchups['Team2Rank']\n",
    "    matchups['Team1RankDiffAbs'] = matchups['Team1RankDiff'].abs()\n",
    "    matchups['Team1RankDiffPct'] = matchups['Team1RankDiff'] / matchups['Team2Rank']\n",
    "    matchups['Team1RankDiffPctAbs'] = matchups['Team1RankDiffPct'].abs()\n",
    "\n",
    "    return matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matchups(games, team_stats, dfs):\n",
    "    # Create labeled matchup data from real games\n",
    "    matchups = games[['Season', 'WTeamID', 'LTeamID']].copy()\n",
    "    matchups['Team1ID'] = matchups['WTeamID']\n",
    "    matchups['Team2ID'] = matchups['LTeamID']\n",
    "    matchups['Team1Won'] = 1\n",
    "\n",
    "    # Merge in features for both teams\n",
    "    for i in [1, 2]:\n",
    "        matchups = matchups.merge(\n",
    "            team_stats,\n",
    "            how='left',\n",
    "            left_on=['Season', f'Team{i}ID'],\n",
    "            right_on=['Season', 'TeamID']\n",
    "        )\n",
    "        matchups = matchups.rename(columns={\n",
    "            'avg_points_for': f'Team{i}_avg_points_for',\n",
    "            'avg_points_against': f'Team{i}_avg_points_against',\n",
    "            'win_pct': f'Team{i}_win_pct',\n",
    "            'avg_margin_of_victory': f'Team{i}_avg_margin_of_victory'  # Add margin of victory\n",
    "        })\n",
    "        matchups.drop(columns=['TeamID'], inplace=True)\n",
    "\n",
    "    # randomly swap Team1 and Team2 \n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Create a random boolean array: True means \"swap\"\n",
    "    swap_mask = np.random.rand(len(matchups)) < 0.5\n",
    "\n",
    "    # Swap team IDs by matching the mask\n",
    "    matchups.loc[swap_mask, ['Team1ID', 'Team2ID']] = matchups.loc[swap_mask, ['Team2ID', 'Team1ID']].values\n",
    "    for feature in ['avg_points_for', 'avg_points_against', 'win_pct']:\n",
    "        team1_feature = f'Team1_{feature}'\n",
    "        team2_feature = f'Team2_{feature}'\n",
    "        matchups.loc[swap_mask, [team1_feature, team2_feature]] = matchups.loc[swap_mask, [team2_feature, team1_feature]].values\n",
    "\n",
    "    # Set the target: 1 if original Team1 won, 0 if swapped\n",
    "    matchups['Team1Won'] = (~swap_mask).astype(int)\n",
    "\n",
    "    # add ordinal rankings to matchups\n",
    "    matchups = merge_rankings(matchups, dfs)\n",
    "\n",
    "    return matchups\n",
    "matchups = generate_matchups(games, team_stats, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2df23dae-bede-41d9-b084-476cd331c981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy with rankings: 0.7592947747\n"
     ]
    }
   ],
   "source": [
    "# Update feature columns to include ranking features\n",
    "feature_cols = [\n",
    "    'Team1_avg_points_for', 'Team1_avg_points_against', 'Team1_win_pct', \n",
    "    'Team2_avg_points_for', 'Team2_avg_points_against', 'Team2_win_pct', \n",
    "    'Team1Rank', 'Team2Rank', 'Team1RankDiff', 'Team1RankDiffAbs', 'Team1RankDiffPct', 'Team1RankDiffPctAbs'\n",
    "]\n",
    "# Build feature matrix and labels\n",
    "X = matchups[feature_cols]\n",
    "y = matchups['Team1Won']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Set Accuracy with rankings: {accuracy:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression model with the ordinal ranking data had an accuracy of 75.93%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Add seed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchups_with_seed_data(matchups, dfs):\n",
    "    # add seed data to matchups\n",
    "    # generate matchups with seed data\n",
    "    seeds = dfs['MNCAATourneySeeds']\n",
    "    seeds['SeedNum'] = seeds['Seed'].str.extract('(\\\\d+)').astype(int)\n",
    "\n",
    "    # Team1 seed\n",
    "    matchups = matchups.merge(seeds[['Season', 'TeamID', 'SeedNum']], left_on=['Season', 'Team1ID'], right_on=['Season', 'TeamID'], how='left')\n",
    "    matchups = matchups.rename(columns={'SeedNum': 'Team1Seed'}).drop(columns=['TeamID'])\n",
    "\n",
    "    # Team2 seed\n",
    "    matchups = matchups.merge(seeds[['Season', 'TeamID', 'SeedNum']], left_on=['Season', 'Team2ID'], right_on=['Season', 'TeamID'], how='left')\n",
    "    matchups = matchups.rename(columns={'SeedNum': 'Team2Seed'}).drop(columns=['TeamID'])\n",
    "\n",
    "    # Fill missing seeds with 17 (meaning worse than 16-seed)\n",
    "    matchups.fillna({'Team1Seed': 17, 'Team2Seed': 17}, inplace=True)\n",
    "\n",
    "\n",
    "    return matchups\n",
    "matchups = generate_matchups(games, team_stats, dfs)\n",
    "seed_matchups = matchups_with_seed_data(matchups, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy with seed data: 0.7591265435\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [\n",
    "    'Team1_avg_points_for', 'Team1_avg_points_against', 'Team1_win_pct', \n",
    "    'Team2_avg_points_for', 'Team2_avg_points_against', 'Team2_win_pct', \n",
    "    'Team1Rank', 'Team2Rank', 'Team1RankDiff', 'Team1RankDiffAbs', 'Team1RankDiffPct', 'Team1RankDiffPctAbs',\n",
    "    'Team1Seed', 'Team2Seed'\n",
    "]\n",
    "X = seed_matchups[feature_cols]\n",
    "y = seed_matchups['Team1Won']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy with seed data: {accuracy:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression model with seed data had an accuracy of 75.91%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5472b-a152-4662-a383-c10f739f5825",
   "metadata": {},
   "source": [
    " The model with the ordinal rankings feature is a better predictor of the outcome of a game, scoring an accuracy of 75.93%, than the model without these features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Random Forest\n",
    "We will see if using the Random Forest Algorithm will produce better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7197291500189259\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# matchups = generate_matchups(games, team_stats, dfs)\n",
    "feature_cols = [\n",
    "    'Team1_avg_points_for', 'Team1_avg_points_against', 'Team1_win_pct', \n",
    "    'Team2_avg_points_for', 'Team2_avg_points_against', 'Team2_win_pct', \n",
    "    'Team1Rank', 'Team2Rank', 'Team1RankDiff', 'Team1RankDiffAbs', 'Team1RankDiffPct', 'Team1RankDiffPctAbs',\n",
    "]\n",
    "# Split data\n",
    "X = matchups[feature_cols]\n",
    "y = matchups['Team1Won']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = rf.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using this random forest model, we get an accuracy of 71.97%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.719140345712243\n"
     ]
    }
   ],
   "source": [
    "# change n_estimators to 300\n",
    "rf = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the n_estimators to 300 decreased the accuracy from 71.97% to 71.91% which is not much difference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
