{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9acb8ae1-0cd8-427f-8bdb-bd7c16b22290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9941e080-e6b4-44ad-962a-512326638bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Paths\n",
    "# Define the root project directory and construct paths to relevant subdirectories\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(os.getcwd())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f5003859-e550-45f4-be2b-d809fe0fc188",
   "metadata": {},
   "outputs": [],
   "source": [
    "basics_dir_path = os.path.join(project_root, 'data', 'section_1_basics')\n",
    "team_box_scores_dir_path = os.path.join(project_root, 'data', 'section_2_team_box_scores')\n",
    "geography_dir_path = os.path.join(project_root, 'data', 'section_3_geography')\n",
    "public_rankings_dir_path = os.path.join(project_root, 'data', 'section_4_public_rankings')\n",
    "supplements_dir_path = os.path.join(project_root, 'data', 'section_5_supplements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4a5e3f5d-4be1-4f92-a75c-09d0341d94e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Completeness Check\n",
    "# Check for missing values and duplicates in all files within a directory\n",
    "def data_completeness(directory_path):\n",
    "    directory = os.listdir(directory_path)\n",
    "    for file in directory:\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.replace(\"\", np.nan, inplace=True)\n",
    "            num_duplicates = df.duplicated().sum()\n",
    "            missing_val_count_by_column = df.isnull().sum()\n",
    "            if missing_val_count_by_column.any():\n",
    "                print(f'Missing values in {file}:\\n{missing_val_count_by_column[missing_val_count_by_column > 0]}')\n",
    "            if num_duplicates > 0:\n",
    "                print(f'Duplicates in {file}: {num_duplicates}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0a86b450-0556-43fc-a719-1feb18b8f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run data validation on all relevant directories\n",
    "data_completeness(basics_dir_path)\n",
    "data_completeness(team_box_scores_dir_path)\n",
    "data_completeness(geography_dir_path)\n",
    "data_completeness(public_rankings_dir_path)\n",
    "data_completeness(supplements_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d3d87308-dc44-40be-bcd1-d2cbefa67d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling Pipeline\n",
    "# Load regular season results for a given gender ('M' or 'W')\n",
    "def load_regular_season_results(gender):\n",
    "    filename = f\"{gender}RegularSeasonCompactResults.csv\"\n",
    "    reg_res = pd.read_csv(os.path.join(basics_dir_path, filename))\n",
    "    return reg_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "efd4991b-9ae6-4379-b1fd-7524e5af82fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tournament seed data and extract numerical seed value\n",
    "def load_seed_data(gender):\n",
    "    filename = f\"{gender}NCAATourneySeeds.csv\"\n",
    "    seeds_df = pd.read_csv(os.path.join(basics_dir_path, filename))\n",
    "    seeds_df['Seed'] = seeds_df['Seed'].str.extract('(\\d+)').astype(int)\n",
    "    return seeds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "03d7995c-b397-480d-9435-93046b298996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all valid matchups for a given season\n",
    "def generate_matchups(team_df, season):\n",
    "    team_ids = team_df['TeamID'].unique()\n",
    "    pairs = [(a, b) for a, b in combinations(team_ids, 2)]\n",
    "    return pd.DataFrame({\n",
    "        'Season': season,\n",
    "        'Team1ID': [min(a, b) for a, b in pairs],\n",
    "        'Team2ID': [max(a, b) for a, b in pairs]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "72f89a25-b9b7-4cf2-a86e-5d007ee841ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate average points and win rate statistics for each team\n",
    "def build_team_stats(df):\n",
    "    winning = df[['Season', 'WTeamID', 'WScore', 'LScore']].rename(\n",
    "        columns={'WTeamID': 'TeamID', 'WScore': 'PointsFor', 'LScore': 'PointsAgainst'})\n",
    "    winning['Win'] = 1\n",
    "\n",
    "    losing = df[['Season', 'LTeamID', 'LScore', 'WScore']].rename(\n",
    "        columns={'LTeamID': 'TeamID', 'LScore': 'PointsFor', 'WScore': 'PointsAgainst'})\n",
    "    losing['Win'] = 0\n",
    "\n",
    "    all_stats = pd.concat([winning, losing])\n",
    "    team_stats = all_stats.groupby(['Season', 'TeamID']).agg(\n",
    "        avg_points_for=('PointsFor', 'mean'),\n",
    "        avg_points_against=('PointsAgainst', 'mean'),\n",
    "        win_pct=('Win', 'mean')\n",
    "    ).reset_index()\n",
    "    return team_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "19882c92-cb24-4139-b33c-bc148c78d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_training_data(gender):\n",
    "    df = load_regular_season_results(gender)\n",
    "    team_stats = build_team_stats(df)\n",
    "    seeds_df = load_seed_data(gender)\n",
    "\n",
    "    # Create labeled matchup data from real games\n",
    "    matchups = df[['Season', 'WTeamID', 'LTeamID']].copy()\n",
    "    matchups['Team1ID'] = matchups['WTeamID']\n",
    "    matchups['Team2ID'] = matchups['LTeamID']\n",
    "    matchups['Team1Won'] = 1\n",
    "\n",
    "    # Merge in features for both teams\n",
    "    for i in [1, 2]:\n",
    "        matchups = matchups.merge(\n",
    "            team_stats,\n",
    "            how='left',\n",
    "            left_on=['Season', f'Team{i}ID'],\n",
    "            right_on=['Season', 'TeamID']\n",
    "        ).merge(\n",
    "            seeds_df,\n",
    "            how='left',\n",
    "            left_on=['Season', f'Team{i}ID'],\n",
    "            right_on=['Season', 'TeamID']\n",
    "        ).rename(columns={\n",
    "            'avg_points_for': f'Team{i}_avg_points_for',\n",
    "            'avg_points_against': f'Team{i}_avg_points_against',\n",
    "            'win_pct': f'Team{i}_win_pct',\n",
    "            'Seed': f'Team{i}_Seed'\n",
    "        }).drop(columns=['TeamID_x', 'TeamID_y'])\n",
    "\n",
    "    # Randomize order of teams to avoid label bias\n",
    "    np.random.seed(42)\n",
    "    swap_mask = np.random.rand(len(matchups)) < 0.5\n",
    "    for col in ['avg_points_for', 'avg_points_against', 'win_pct', 'Seed']:\n",
    "        matchups.loc[swap_mask, [f'Team1_{col}', f'Team2_{col}']] = matchups.loc[swap_mask, [f'Team2_{col}', f'Team1_{col}']].values\n",
    "    matchups['Team1Won'] = (~swap_mask).astype(int)\n",
    "\n",
    "    # Select features and train/test split\n",
    "    feature_cols = [\n",
    "        'Team1_avg_points_for', 'Team1_avg_points_against', 'Team1_win_pct', 'Team1_Seed',\n",
    "        'Team2_avg_points_for', 'Team2_avg_points_against', 'Team2_win_pct', 'Team2_Seed']\n",
    "\n",
    "    X = matchups[feature_cols].fillna(-1)\n",
    "    y = matchups['Team1Won']\n",
    "    return X, y, team_stats, seeds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8f867e59-66b9-470a-b082-4975d125ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y):\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_valid_scaled = scaler.transform(X_valid)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return scaler, X_train_scaled, y_train, X_valid_scaled, y_valid, X_test_scaled, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ea3223cf-d49c-47ff-b603-608ca222df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model(X_train_scaled, y_train, X_valid_scaled, y_valid):\n",
    "    model = TabNetClassifier()\n",
    "    model.fit(X_train=X_train_scaled, y_train=y_train, eval_set=[(X_valid_scaled, y_valid)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "794d2eac-737b-4372-8ac8-88d5b777683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_stats(model, X_test_scaled, y_test, gender):\n",
    "    pred = model.predict(X_test_scaled)\n",
    "    probs = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    print(f\"{gender} Model Accuracy:\", accuracy_score(y_test, pred))\n",
    "    brier = brier_score_loss(y_test, probs)\n",
    "    print(f\"{gender} Model Brier Score: {brier:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "2cfddbe3-dffc-4e7a-add6-12f29f8f8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and predict win probabilities for generated matchups\n",
    "def create_features_and_predict(matchups, team_stats, seeds_df, model, scaler):\n",
    "    for i in [1, 2]:\n",
    "        matchups = matchups.merge(\n",
    "            team_stats,\n",
    "            how='left',\n",
    "            left_on=['Season', f'Team{i}ID'],\n",
    "            right_on=['Season', 'TeamID']\n",
    "        ).merge(\n",
    "            seeds_df,\n",
    "            how='left',\n",
    "            left_on=['Season', f'Team{i}ID'],\n",
    "            right_on=['Season', 'TeamID']\n",
    "        ).rename(columns={\n",
    "            'avg_points_for': f'Team{i}_avg_points_for',\n",
    "            'avg_points_against': f'Team{i}_avg_points_against',\n",
    "            'win_pct': f'Team{i}_win_pct',\n",
    "            'Seed': f'Team{i}_Seed'\n",
    "        }).drop(columns=['TeamID_x', 'TeamID_y'])\n",
    "\n",
    "    features = matchups[[\n",
    "        'Team1_avg_points_for', 'Team1_avg_points_against', 'Team1_win_pct', 'Team1_Seed',\n",
    "        'Team2_avg_points_for', 'Team2_avg_points_against', 'Team2_win_pct', 'Team2_Seed']].fillna(-1)\n",
    "\n",
    "    features_scaled = scaler.transform(features)\n",
    "    probs = model.predict_proba(features_scaled)[:, 1]\n",
    "\n",
    "    matchups['ID'] = matchups.apply(lambda row: f\"2025_{int(row.Team1ID)}_{int(row.Team2ID)}\", axis=1)\n",
    "    matchups['Pred'] = probs\n",
    "    return matchups[['ID', 'Pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "fa36da72-b8b8-4769-8e11-0aeb3018dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pipeline function to train both models and generate the submission file\n",
    "def create_submission():\n",
    "    # Train men's model and predict\n",
    "    m_X, m_y, m_stats, m_seeds = prep_training_data('M')\n",
    "    m_scaler, m_X_train_scaled, m_y_train, m_X_valid_scaled, m_y_valid, m_X_test_scaled, m_y_test = split_data(m_X, m_y)\n",
    "    m_model = create_and_train_model(m_X_train_scaled, m_y_train, m_X_valid_scaled, m_y_valid)\n",
    "    model_stats(m_model, m_X_test_scaled, m_y_test, 'M')\n",
    "    m_teams = pd.read_csv(os.path.join(basics_dir_path, \"MTeams.csv\"))\n",
    "    m_matchups = generate_matchups(m_teams, 2025)\n",
    "    m_preds = create_features_and_predict(m_matchups, m_stats, m_seeds, m_model, m_scaler)\n",
    "\n",
    "    # Train women's model and predict\n",
    "    w_X, w_y, w_stats, w_seeds = prep_training_data('W')\n",
    "    w_scaler, w_X_train_scaled, w_y_train, w_X_valid_scaled, w_y_valid, w_X_test_scaled, w_y_test = split_data(w_X, w_y)\n",
    "    w_model = create_and_train_model(w_X_train_scaled, w_y_train, w_X_valid_scaled, w_y_valid)\n",
    "    model_stats(w_model, w_X_test_scaled, w_y_test, 'W')\n",
    "    w_teams = pd.read_csv(os.path.join(basics_dir_path, \"WTeams.csv\"))\n",
    "    w_matchups = generate_matchups(w_teams, 2025)\n",
    "    w_preds = create_features_and_predict(w_matchups, w_stats, w_seeds, w_model, w_scaler)\n",
    "\n",
    "    # Save combined predictions to submission file\n",
    "    submission = pd.concat([m_preds, w_preds])\n",
    "    submission.to_csv(f\"submission.csv\", index=False)\n",
    "    print(f\"Submission file saved as submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ad7e9870-6254-401a-b779-ad6765a6ed3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kingc\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.55229 | val_0_auc: 0.81531 |  0:00:04s\n",
      "epoch 1  | loss: 0.52422 | val_0_auc: 0.81566 |  0:00:10s\n",
      "epoch 2  | loss: 0.52333 | val_0_auc: 0.81627 |  0:00:15s\n",
      "epoch 3  | loss: 0.5223  | val_0_auc: 0.81595 |  0:00:20s\n",
      "epoch 4  | loss: 0.52162 | val_0_auc: 0.81453 |  0:00:25s\n",
      "epoch 5  | loss: 0.52182 | val_0_auc: 0.81785 |  0:00:30s\n",
      "epoch 6  | loss: 0.51997 | val_0_auc: 0.81775 |  0:00:35s\n",
      "epoch 7  | loss: 0.51961 | val_0_auc: 0.81758 |  0:00:39s\n",
      "epoch 8  | loss: 0.51951 | val_0_auc: 0.81817 |  0:00:44s\n",
      "epoch 9  | loss: 0.51911 | val_0_auc: 0.81866 |  0:00:49s\n",
      "epoch 10 | loss: 0.51838 | val_0_auc: 0.81853 |  0:00:54s\n",
      "epoch 11 | loss: 0.51875 | val_0_auc: 0.81903 |  0:00:59s\n",
      "epoch 12 | loss: 0.51893 | val_0_auc: 0.81909 |  0:01:04s\n",
      "epoch 13 | loss: 0.51788 | val_0_auc: 0.81867 |  0:01:09s\n",
      "epoch 14 | loss: 0.51784 | val_0_auc: 0.81961 |  0:01:14s\n",
      "epoch 15 | loss: 0.51725 | val_0_auc: 0.81939 |  0:01:19s\n",
      "epoch 16 | loss: 0.5179  | val_0_auc: 0.81916 |  0:01:24s\n",
      "epoch 17 | loss: 0.51809 | val_0_auc: 0.8183  |  0:01:29s\n",
      "epoch 18 | loss: 0.51728 | val_0_auc: 0.81923 |  0:01:34s\n",
      "epoch 19 | loss: 0.51726 | val_0_auc: 0.81873 |  0:01:38s\n",
      "epoch 20 | loss: 0.51717 | val_0_auc: 0.81904 |  0:01:43s\n",
      "epoch 21 | loss: 0.51747 | val_0_auc: 0.81978 |  0:01:48s\n",
      "epoch 22 | loss: 0.51706 | val_0_auc: 0.81812 |  0:01:53s\n",
      "epoch 23 | loss: 0.51752 | val_0_auc: 0.81972 |  0:01:58s\n",
      "epoch 24 | loss: 0.51679 | val_0_auc: 0.82002 |  0:02:03s\n",
      "epoch 25 | loss: 0.51578 | val_0_auc: 0.82081 |  0:02:08s\n",
      "epoch 26 | loss: 0.51643 | val_0_auc: 0.82064 |  0:02:13s\n",
      "epoch 27 | loss: 0.51724 | val_0_auc: 0.81956 |  0:02:18s\n",
      "epoch 28 | loss: 0.51608 | val_0_auc: 0.82024 |  0:02:23s\n",
      "epoch 29 | loss: 0.51591 | val_0_auc: 0.82045 |  0:02:28s\n",
      "epoch 30 | loss: 0.51566 | val_0_auc: 0.82111 |  0:02:33s\n",
      "epoch 31 | loss: 0.5159  | val_0_auc: 0.82032 |  0:02:38s\n",
      "epoch 32 | loss: 0.51601 | val_0_auc: 0.8208  |  0:02:43s\n",
      "epoch 33 | loss: 0.51544 | val_0_auc: 0.82015 |  0:02:48s\n",
      "epoch 34 | loss: 0.51594 | val_0_auc: 0.82059 |  0:02:53s\n",
      "epoch 35 | loss: 0.51593 | val_0_auc: 0.82063 |  0:02:58s\n",
      "epoch 36 | loss: 0.51529 | val_0_auc: 0.82097 |  0:03:03s\n",
      "epoch 37 | loss: 0.51546 | val_0_auc: 0.82085 |  0:03:08s\n",
      "epoch 38 | loss: 0.51557 | val_0_auc: 0.82088 |  0:03:13s\n",
      "epoch 39 | loss: 0.51489 | val_0_auc: 0.82037 |  0:03:18s\n",
      "epoch 40 | loss: 0.51514 | val_0_auc: 0.82075 |  0:03:23s\n",
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 30 and best_val_0_auc = 0.82111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kingc\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M Model Accuracy: 0.7438708339812368\n",
      "M Model Brier Score: 0.1711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kingc\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.52935 | val_0_auc: 0.83873 |  0:00:03s\n",
      "epoch 1  | loss: 0.48814 | val_0_auc: 0.84863 |  0:00:07s\n",
      "epoch 2  | loss: 0.48158 | val_0_auc: 0.84888 |  0:00:11s\n",
      "epoch 3  | loss: 0.47916 | val_0_auc: 0.85137 |  0:00:15s\n",
      "epoch 4  | loss: 0.47799 | val_0_auc: 0.85236 |  0:00:18s\n",
      "epoch 5  | loss: 0.47736 | val_0_auc: 0.85191 |  0:00:22s\n",
      "epoch 6  | loss: 0.47591 | val_0_auc: 0.85274 |  0:00:25s\n",
      "epoch 7  | loss: 0.47495 | val_0_auc: 0.8533  |  0:00:29s\n",
      "epoch 8  | loss: 0.47524 | val_0_auc: 0.85072 |  0:00:32s\n",
      "epoch 9  | loss: 0.47485 | val_0_auc: 0.853   |  0:00:35s\n",
      "epoch 10 | loss: 0.47419 | val_0_auc: 0.85324 |  0:00:39s\n",
      "epoch 11 | loss: 0.47418 | val_0_auc: 0.85385 |  0:00:42s\n",
      "epoch 12 | loss: 0.47258 | val_0_auc: 0.85412 |  0:00:46s\n",
      "epoch 13 | loss: 0.47415 | val_0_auc: 0.85319 |  0:00:49s\n",
      "epoch 14 | loss: 0.47297 | val_0_auc: 0.85409 |  0:00:53s\n",
      "epoch 15 | loss: 0.47309 | val_0_auc: 0.8536  |  0:00:56s\n",
      "epoch 16 | loss: 0.47247 | val_0_auc: 0.85403 |  0:01:00s\n",
      "epoch 17 | loss: 0.4728  | val_0_auc: 0.85421 |  0:01:03s\n",
      "epoch 18 | loss: 0.47204 | val_0_auc: 0.8539  |  0:01:06s\n",
      "epoch 19 | loss: 0.47177 | val_0_auc: 0.85492 |  0:01:10s\n",
      "epoch 20 | loss: 0.47204 | val_0_auc: 0.85399 |  0:01:13s\n",
      "epoch 21 | loss: 0.47077 | val_0_auc: 0.85483 |  0:01:17s\n",
      "epoch 22 | loss: 0.47205 | val_0_auc: 0.85498 |  0:01:20s\n",
      "epoch 23 | loss: 0.47108 | val_0_auc: 0.85476 |  0:01:24s\n",
      "epoch 24 | loss: 0.47126 | val_0_auc: 0.85424 |  0:01:27s\n",
      "epoch 25 | loss: 0.47036 | val_0_auc: 0.85426 |  0:01:30s\n",
      "epoch 26 | loss: 0.47052 | val_0_auc: 0.8549  |  0:01:34s\n",
      "epoch 27 | loss: 0.47045 | val_0_auc: 0.85401 |  0:01:37s\n",
      "epoch 28 | loss: 0.4704  | val_0_auc: 0.85488 |  0:01:41s\n",
      "epoch 29 | loss: 0.47226 | val_0_auc: 0.85412 |  0:01:44s\n",
      "epoch 30 | loss: 0.47084 | val_0_auc: 0.85427 |  0:01:48s\n",
      "epoch 31 | loss: 0.47015 | val_0_auc: 0.85452 |  0:01:51s\n",
      "epoch 32 | loss: 0.47059 | val_0_auc: 0.85411 |  0:01:55s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_auc = 0.85498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kingc\\anaconda3\\envs\\myenv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W Model Accuracy: 0.7679705174049478\n",
      "W Model Brier Score: 0.1554\n",
      "Submission file saved as submission.csv\n"
     ]
    }
   ],
   "source": [
    "create_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47b32ad-c8a3-4cf6-bf7d-3e23bf2e1572",
   "metadata": {},
   "source": [
    "Actual Kaggle Submission Brier Score: 0.13163"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3cbdb-5354-480d-a2f1-797ef26202ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
